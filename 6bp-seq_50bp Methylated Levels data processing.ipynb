{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf146f-cd05-46db-8153-c1e9f2acb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have Decided to use 50bp for the window size\n",
    "# This is the process to convert txt files into bedgraph files \n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def batch_generate_50bp_level(input_dir=\".\", bin_size=50, min_cov=10):\n",
    "    # 1. æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶\n",
    "    txt_files = glob.glob(os.path.join(input_dir, \"*.txt\"))\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(\"æœªæ‰¾åˆ°ä»»ä½• .txt æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚\")\n",
    "        return\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        output_bg = txt_file.replace(\".txt\", f\".bin{bin_size}.bedgraph\")\n",
    "        print(f\"--- æ­£åœ¨å¤„ç†: {os.path.basename(txt_file)} ---\")\n",
    "        \n",
    "        # 2. è¯»å–æ•°æ® (é’ˆå¯¹ä½ çš„ CX report æ ¼å¼)\n",
    "        # 0:chr, 1:pos, 3:met, 4:unmet\n",
    "        try:\n",
    "            # ä½¿ç”¨ chunk æ¨¡å¼è¯»å–ä»¥é˜²å†…å­˜å´©æºƒ\n",
    "            chunks = pd.read_csv(txt_file, sep='\\t', header=None, \n",
    "                                 usecols=[0, 1, 3, 4], \n",
    "                                 names=['chr', 'pos', 'met', 'unmet'],\n",
    "                                 chunksize=2000000)\n",
    "            \n",
    "            # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ª chunk èšåˆåçš„ç»“æœ\n",
    "            aggregated_data = []\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                # è®¡ç®— 50bp bin\n",
    "                chunk['bin_start'] = (chunk['pos'] // bin_size) * bin_size\n",
    "                # ç»„å†…æ±‚å’Œ\n",
    "                binned_chunk = chunk.groupby(['chr', 'bin_start']).agg({'met': 'sum', 'unmet': 'sum'}).reset_index()\n",
    "                aggregated_data.append(binned_chunk)\n",
    "            \n",
    "            # 3. äºŒæ¬¡èšåˆï¼šç¡®ä¿è·¨ chunk çš„åŒä¸€ä¸ª bin è¢«åˆå¹¶\n",
    "            full_binned = pd.concat(aggregated_data).groupby(['chr', 'bin_start']).agg({'met': 'sum', 'unmet': 'sum'}).reset_index()\n",
    "            \n",
    "            # 4. è®¡ç®— Level å¹¶è¿‡æ»¤\n",
    "            full_binned['total'] = full_binned['met'] + full_binned['unmet']\n",
    "            valid = full_binned[full_binned['total'] >= min_cov].copy()\n",
    "            valid['level'] = (valid['met'] / valid['total']).round(4) # ä¿ç•™4ä½å°æ•°\n",
    "            valid['bin_end'] = valid['bin_start'] + bin_size\n",
    "            \n",
    "            # 5. æŒ‰ç…§ BedGraph æ ‡å‡†æ ¼å¼æ’åºå¹¶ä¿å­˜\n",
    "            # æ ‡å‡†æ ¼å¼ï¼šchr, start, end, value\n",
    "            valid = valid.sort_values(['chr', 'bin_start'])\n",
    "            valid[['chr', 'bin_start', 'bin_end', 'level']].to_csv(\n",
    "                output_bg, sep='\\t', index=False, header=False\n",
    "            )\n",
    "            \n",
    "            print(f\"æˆåŠŸå¯¼å‡º: {output_bg}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç† {txt_file} æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# åœ¨ Jupyter ä¸­ç›´æ¥è¿è¡Œï¼š\n",
    "batch_generate_50bp_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5d0ed-2c3c-4700-ba3d-bae528063105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the file to convert bedgraph into bigwig files\n",
    "import pyBigWig\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def batch_convert_to_bigwig(chrom_sizes_file, input_pattern=\"*.bin50.bedgraph\"):\n",
    "    # 1. åŠ è½½æŸ“è‰²ä½“å¤§å°å¹¶è®°å½•é¡ºåº (BigWig å¯¹é¡ºåºæœ‰ä¸¥æ ¼è¦æ±‚)\n",
    "    print(\"æ­£åœ¨åŠ è½½æŸ“è‰²ä½“å¤§å°...\")\n",
    "    chrom_order = []\n",
    "    chrom_sizes_dict = {}\n",
    "    with open(chrom_sizes_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                chrom_order.append(parts[0]) \n",
    "                chrom_sizes_dict[parts[0]] = int(parts[1])\n",
    "    \n",
    "    # 2. æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶\n",
    "    files = glob.glob(input_pattern)\n",
    "    if not files:\n",
    "        print(f\"æœªæ‰¾åˆ°åŒ¹é… '{input_pattern}' çš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚\")\n",
    "        return\n",
    "\n",
    "    print(f\"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶å‡†å¤‡è½¬æ¢...\")\n",
    "\n",
    "    for bedgraph_file in files:\n",
    "        # è®¾å®šè¾“å‡ºæ–‡ä»¶å (æŠŠ .bedgraph æ›¿æ¢ä¸º .bw)\n",
    "        output_bw = bedgraph_file.replace(\".bedgraph\", \".bw\")\n",
    "        \n",
    "        print(f\"\\n--- æ­£åœ¨å¤„ç†: {bedgraph_file} ---\")\n",
    "        \n",
    "        try:\n",
    "            # åˆ›å»º BigWig\n",
    "            bw = pyBigWig.open(output_bw, \"w\")\n",
    "            bw.addHeader([(chrom, chrom_sizes_dict[chrom]) for chrom in chrom_order])\n",
    "            \n",
    "            # è¯»å–æ•°æ®\n",
    "            df = pd.read_csv(bedgraph_file, sep='\\t', header=None)\n",
    "            df.columns = ['chrom', 'start', 'end', 'value']\n",
    "            \n",
    "            # æŒ‰ç…§ chrom.sizes çš„é¡ºåºå†™å…¥ (BigWig å¿…é¡»æŒ‰ header é¡ºåºå†™å…¥)\n",
    "            for chrom in chrom_order:\n",
    "                group = df[df['chrom'] == chrom]\n",
    "                if not group.empty:\n",
    "                    group = group.sort_values('start')\n",
    "                    # å†™å…¥ BigWig\n",
    "                    bw.addEntries(\n",
    "                        group['chrom'].tolist(),\n",
    "                        group['start'].tolist(),\n",
    "                        ends=group['end'].tolist(),\n",
    "                        values=group['value'].tolist()\n",
    "                    )\n",
    "            \n",
    "            bw.close()\n",
    "            print(f\"âœ… è½¬æ¢æˆåŠŸ: {output_bw}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç† {bedgraph_file} æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# --- æ‰§è¡Œè®¾ç½® ---\n",
    "# ç¡®ä¿ä½ çš„ hg38.chrom.sizes åœ¨å½“å‰ç›®å½•ä¸‹\n",
    "my_sizes = 'hg38.chrom.sizes' \n",
    "\n",
    "# æ‰§è¡Œæ‰¹é‡è½¬æ¢\n",
    "batch_convert_to_bigwig(my_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adb929-2cd0-4a96-b1b5-90aeb23fbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The process that deeptools can compare two bw files based on set window size\n",
    "bigwigCompare -b1 675722-0427605773.targeted.GRCh38Decoy.markdup.CG.num_modc_cxreport.bin50.bw -b2 675723-0427605772.targeted.GRCh38Decoy.markdup.CG.num_modc_cxreport.bin50.bw \\\n",
    "    -o gCGG_vs_gScr_modc_diff.bw \\\n",
    "    --operation log2 \\\n",
    "    --pseudocount 1.0 \\\n",
    "    --binSize 50 \\\n",
    "    --p 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920353a-98f5-42f7-ba33-2a45f0302180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The way we unzip CGG-CCG gz file \n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "input_file = 'CGG-CCG_repeatMasker.hg38.sorted.bed.gz'\n",
    "output_file = 'CGG-CCG_repeatMasker.hg38.sorted.bed'\n",
    "\n",
    "print(f\"ğŸ“‚ æ­£åœ¨è§£å‹ {input_file}...\")\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "print(f\"âœ… è§£å‹å®Œæˆï¼æ–°æ–‡ä»¶ä½äº: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb78c24-d994-455e-8e61-03c9111335be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The way we try to plot heatmap with CGG-CCG at the center  \n",
    "computeMatrix reference-point --referencePoint center \\\n",
    "  -S gCGG_vs_gScr_mc_diff.bw \\\n",
    "  -R CGG-CCG_repeatMasker.hg38.sorted.bed \\\n",
    "  -b 1000 -a 1000 \\\n",
    "  -bs 50 \\\n",
    "  -o gCGG_vs_gScr_5mc_50bp.gz \\\n",
    "  --missingDataAsZero\n",
    "\n",
    "# ç»˜åˆ¶æ²¡æœ‰èšç±»çš„ç»„åˆå›¾ï¼ˆProfile + Heatmapï¼‰\n",
    "plotHeatmap -m gCGG_vs_gScr_5mc_50bp.gz \\\n",
    "  -out gCGG_vs_gScr_5mc_50bp.pdf \\\n",
    "  --colorMap RdPu \\\n",
    "  --zMin 0 --zMax 0.3 \\\n",
    "  --regionsLabel \"Total CGG Repeats\" \\\n",
    "  --refPointLabel \"CGG\" \\\n",
    "  --heatmapHeight 25 \\\n",
    "  --plotTitle \"Global 6bp-seq Signal at CGG Sites (50bp)\" \\\n",
    "  --whatToShow \"plot, heatmap and colorbar\" \\\n",
    "  --averageType mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7de08e-f24f-454b-849c-74615222f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# or with K-means \n",
    "plotHeatmap -m gCGG_vs_gScr_5mc_50bp.gz \\\n",
    "  -out gCGG_vs_gScr_5mc_50bp_kmeans5.pdf \\     # we can change the k values\n",
    "  --kmeans 5 \\\n",
    "  --colorMap RdPu \\\n",
    "  --zMin 0 --zMax 0.3 \\\n",
    "  --regionsLabel \"Cluster 1\" \"Cluster 2\" \"Cluster 3\" \"Cluster 4\" \"Cluster 5\" \\\n",
    "  --refPointLabel \"CGG\" \\\n",
    "  --heatmapHeight 25 \\\n",
    "  --plotTitle \"Unbiased 6bp-seq Analysis (50bp bins)\" \\\n",
    "  --whatToShow \"plot, heatmap and colorbar\" \\\n",
    "  --averageType mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
